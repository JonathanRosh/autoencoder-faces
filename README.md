ğŸ§  Autoencoder for Face Reconstruction (FFHQ Dataset)

This project implements and compares 10 convolutional autoencoder architectures trained on a 256Ã—256 subset of the FFHQ face dataset.
The goal is to understand how changes in latent dimensionality and channel width affect reconstruction quality.

All models were trained on a CUDA GPU using PyTorch, and evaluated using both quantitative metrics and visual reconstructions.

ğŸš€ Features

10 Autoencoder variants (different latent sizes + channel widths)

GPU-accelerated training (CUDA)

Training/validation split: first 1,000 images = validation

Loss metric: MSE

Evaluation metrics:

Mean MSE

PSNR

Visual comparisons: â€œBefore vs Afterâ€

Loss curve plots

Training history saved for each model

Clean, well-structured codebase

ğŸ— Project Structure
autoencoder_project/
â”‚
â”œâ”€â”€ config.py              # all hyperparameters + model variants
â”œâ”€â”€ model.py               # convolutional autoencoder class
â”œâ”€â”€ train.py               # training loop + dataloaders + history logging
â”œâ”€â”€ main.py                # trains a single model end-to-end
â”œâ”€â”€ evaluate.py            # evaluate a model + compute PSNR + save image grids
â”œâ”€â”€ evaluate_all.py        # evaluate all models and export CSV summary
â”œâ”€â”€ plot_losses.py         # generate loss curves from logs
â”‚
â”œâ”€â”€ models/                # saved checkpoints (.pt)
â”œâ”€â”€ logs/                  # training histories
â”œâ”€â”€ eval_outputs/          # before/after images + evaluation_summary.csv
â”œâ”€â”€ loss_plots/            # loss curves for each model
â”‚
â””â”€â”€ README.md

ğŸ”§ Installation & Setup
git clone https://github.com/YOUR_USERNAME/autoencoder-faces.git
cd autoencoder-faces
pip install -r requirements.txt   # (optional, if you create one)


This project is written for Python 3.8+ and PyTorch with CUDA.

ğŸ‹ï¸ Training a Model

Train any model by name:

python main.py --model-name ae_latent_64


All checkpoints are saved automatically into:

models/
logs/

ğŸ“Š Evaluating a Model

Run evaluation + visual outputs:

python evaluate.py --model-name ae_latent_64


This will produce:

a grid image of 16 original/reconstructed pairs

printed metrics (MSE + PSNR)

Visuals are stored in:

eval_outputs/

ğŸ“ˆ Evaluate All Models

To evaluate all 10 models at once:

python evaluate_all.py


This generates:

eval_outputs/evaluation_summary.csv

ğŸ“‰ Plot Loss Curves
python plot_losses.py


Outputs go to:

loss_plots/

ğŸ“ Results (Summary)

Best performing model:

Model name	MSE	PSNR (dB)
ae_wide_64	0.0284	15.47

Worst performing:

Model name	MSE	PSNR (dB)
ae_latent_4	0.1492	8.26

Reconstruction quality strongly correlates with:

wider networks â†’ better detail

larger latent dimensions â†’ fewer artifacts

tiny latent spaces â†’ blurry, low-detail reconstructions

ğŸ“„ Project Report

A full academic-style PDF explaining the architecture, experiments, and results is included.

ğŸ‘¥ Authors

Yonatan Rosh
Uri Ben Dor
Reshit Carmel
